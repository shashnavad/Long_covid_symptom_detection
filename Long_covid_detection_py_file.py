# -*- coding: utf-8 -*-
"""Long-covid-detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jvgms_5_Vu9MjgwHavMGwsZ-_G34q3wg
"""

import re
import numpy as np
import pandas as pd
import pickle
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem.wordnet import WordNetLemmatizer
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

metadata_path = 'D:/Documents/Project-Long covid detection/C-19_metadata.csv/metadata.csv'
meta_df = pd.read_csv(metadata_path, dtype={'doi': str})
meta_df = meta_df[meta_df.publish_time>"2020-01-01"][['cord_uid','doi','title','abstract', 'publish_time', 'authors', 'journal', 'pdf_json_files']].reset_index(drop=True)
meta_df.rename(columns={'cord_uid':'paper_id'}, inplace=True)
meta_df.head(3)

print('{} Total papers in original df'.format(meta_df.shape[0]))
print('{} Papers with abstract'.format(len(meta_df[meta_df['abstract'].notnull()])))

def minimal_clean_text(text):
    text = text.lower()
    text = text.replace('-', ' ')
    text = text.replace('â€“', ' ')
    return text

def preproc_text(text):
    lemmatizer = WordNetLemmatizer()
    text = re.sub(r'[^A-Za-z ]+', '', text)
    text = minimal_clean_text(text)
    stop_words = set(stopwords.words('english'))
    stop_words.add('long')
    stop_words.add('covid')
    stop_words.add('infection')
    word_tokens = word_tokenize(text)
    cleaned_text = [lemmatizer.lemmatize(token) for token in word_tokens if token not in stop_words and len(token)>1]
    return cleaned_text

# Initial processing

meta_df = meta_df.dropna(subset=['abstract'])
meta_df['abstract'] = meta_df['abstract'].apply(minimal_clean_text)
meta_df.head(3)

# Extract only those abstracts which contain long covid
keywords = ['long covid',
            'post covid 19 syndrome',
            'post covid 19 condition',
            'post acute sequelae of covid 19',
            'chronic covid syndrome']

for keyword in keywords:
    meta_df[keyword.replace(' ', '_')] = meta_df['abstract'].apply(lambda x: True if keyword in x else False)
long_covid_query = '==True or '.join(keyword.replace(' ', '_') for keyword in keywords) + '==True'
long_covid_df = meta_df.query(long_covid_query).reset_index(drop=True)
long_covid_df['tokenized_abstract'] = long_covid_df['abstract'].apply(preproc_text)

print('{:.2f}% of the abstracts contain the long covid related phrases'.format(long_covid_df.shape[0]/meta_df.shape[0]*100))
print('{} papers in total'.format(long_covid_df.shape[0]))

long_covid_df['tokenized_abstract'].head()

list_of_all_words = []
for i in long_covid_df['tokenized_abstract']:
    for word in i:
        list_of_all_words.append(word)
print(list_of_all_words)

# Simple word query
print(f'"male" appears {list_of_all_words.count("male")} time(s)')
print(f'"female" appears {list_of_all_words.count("female")} time(s)')
print(f'"old" appears {list_of_all_words.count("old")} time(s)')
print(f'"youth" appears {list_of_all_words.count("diabetic")} time(s)')

df = pd.value_counts(np.array(list_of_all_words))
df.head(20)

# Positional Posting list
line = len(long_covid_df['tokenized_abstract'])
pos_index = {}
fileno = 0
for i in range(line):
    for pos,term in enumerate(long_covid_df['tokenized_abstract'][i]):
        if term in pos_index:
            pos_index[term][0] = pos_index[term][0] + 1
            if fileno in pos_index[term][1]:
                pos_index[term][1][fileno].append(pos)
            else:
                pos_index[term][1][fileno] = [pos]
        else:
                        pos_index[term] = []
                        pos_index[term].append(1)
                        pos_index[term].append({})
                        pos_index[term][1][fileno] = [pos]
    fileno += 1

for key, value in pos_index.items():
    print(key, ' : ', value)

def get_positions(posting_values, doc):
    for key, value in posting_values.items():
        if key == doc:
            return value
    return []

def gen_init_set_matchings(word):
    init = []
    word_postings = pos_index[word][1]
    for key, value in word_postings.items():
        for val in value:
            init.append((key, val))
    return init

def match_positional_index(init, b):
    matched_docs = []
    for p in init:
        doc = p[0]
        pos = p[1]
        count = 0

        for k in b:
            pos = pos+1
            k_pos = pos_index[k][1]
            docs_list = [key for key, value in k_pos.items()]
            if doc in docs_list:
                doc_positions = get_positions(k_pos, doc)
                if pos in doc_positions:
                    count += 1
                else:
                    count += 1
                    break

            if count == len(b):
                matched_docs.append(p[0])
    return set(matched_docs)

def run_query(query_tokens):
    
    init_word = query_tokens[0]
    
    if(len(query_tokens) == 1):
        k_pos = pos_index[init_word][1]
        docs_list = [key for key, value in k_pos.items()] 
        return set(docs_list)
    
    init_matches = gen_init_set_matchings(init_word)    
    
    query_tokens.pop(0)
    total_matched_docs = match_positional_index(init_matches, query_tokens)
    return total_matched_docs

query1 = ['coronary', 'heart', 'disease']

lists = run_query(query1)
print(lists)

query2 = ['high', 'blood', 'pressure']

lists = run_query(query2)
print(lists)

query3 = ['suffering', 'diabetes']

lists = run_query(query3)
print(lists)